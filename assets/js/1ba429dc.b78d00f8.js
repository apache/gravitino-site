"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[80433],{20527:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>c});var r=t(79474);const s={},a=r.createContext(s);function i(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(a.Provider,{value:n},e.children)}},36396:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>c,toc:()=>d});var r=t(13274),s=t(20527);const a={title:"Spark connector JDBC catalog",slug:"/spark-connector/spark-catalog-jdbc",keyword:"spark connector jdbc catalog",license:"This software is licensed under the Apache License version 2."},i=void 0,c={id:"spark-connector/spark-catalog-jdbc",title:"Spark connector JDBC catalog",description:"The Apache Gravitino Spark connector offers the capability to read JDBC tables, with the metadata managed by the Gravitino server. To enable the use of the JDBC catalog within the Spark connector, you must download the jdbc driver jar which you used to Spark classpath.",source:"@site/versioned_docs/version-0.9.0-incubating/spark-connector/spark-catalog-jdbc.md",sourceDirName:"spark-connector",slug:"/spark-connector/spark-catalog-jdbc",permalink:"/docs/0.9.0-incubating/spark-connector/spark-catalog-jdbc",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/gravitino-site/tree/main/docs/spark-connector/spark-catalog-jdbc.md",tags:[],version:"0.9.0-incubating",frontMatter:{title:"Spark connector JDBC catalog",slug:"/spark-connector/spark-catalog-jdbc",keyword:"spark connector jdbc catalog",license:"This software is licensed under the Apache License version 2."},sidebar:"docs",previous:{title:"Paimon",permalink:"/docs/0.9.0-incubating/spark-connector/spark-catalog-paimon"},next:{title:"Spark authentication",permalink:"/docs/0.9.0-incubating/spark-connector/spark-authentication"}},o={},d=[{value:"Capabilities",id:"capabilities",level:2},{value:"Support DML and DDL operations:",id:"support-dml-and-ddl-operations",level:4},{value:"Not supported operations:",id:"not-supported-operations",level:4},{value:"SQL example",id:"sql-example",level:2},{value:"Catalog properties",id:"catalog-properties",level:2}];function l(e){const n={admonition:"admonition",code:"code",h2:"h2",h4:"h4",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"The Apache Gravitino Spark connector offers the capability to read JDBC tables, with the metadata managed by the Gravitino server. To enable the use of the JDBC catalog within the Spark connector, you must download the jdbc driver jar which you used to Spark classpath."}),"\n",(0,r.jsx)(n.h2,{id:"capabilities",children:"Capabilities"}),"\n",(0,r.jsx)(n.p,{children:"Supports MySQL and PostgreSQL. For OceanBase which is compatible with Mysql Dialects could use Mysql driver and Mysql Dialects as a trackoff way. But for Doris which do not support MySQL Dialects, are not currently supported."}),"\n",(0,r.jsx)(n.h4,{id:"support-dml-and-ddl-operations",children:"Support DML and DDL operations:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"CREATE TABLE"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"DROP TABLE"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"ALTER TABLE"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"SELECT"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"INSERT"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"JDBCTable does not support distributed transaction. When writing data to RDBMS, each task is an independent transaction. If some tasks of spark succeed and some tasks fail, dirty data is generated."})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"not-supported-operations",children:"Not supported operations:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"UPDATE"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"DELETE"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"TRUNCATE"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sql-example",children:"SQL example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"-- Suppose mysql_a is the mysql catalog name managed by Gravitino\nUSE mysql_a;\n\nCREATE DATABASE IF NOT EXISTS mydatabase;\nUSE mydatabase;\n\nCREATE TABLE IF NOT EXISTS employee (\n  id bigint,\n  name string,\n  department string,\n  hire_date timestamp\n)\nDESC TABLE EXTENDED employee;\n\nINSERT INTO employee\nVALUES\n(1, 'Alice', 'Engineering', TIMESTAMP '2021-01-01 09:00:00'),\n(2, 'Bob', 'Marketing', TIMESTAMP '2021-02-01 10:30:00'),\n(3, 'Charlie', 'Sales', TIMESTAMP '2021-03-01 08:45:00');\n\nSELECT * FROM employee WHERE date(hire_date) = '2021-01-01';\n\n\n"})}),"\n",(0,r.jsx)(n.h2,{id:"catalog-properties",children:"Catalog properties"}),"\n",(0,r.jsx)(n.p,{children:"Gravitino spark connector will transform below property names which are defined in catalog properties to Spark JDBC connector configuration."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Gravitino catalog property name"}),(0,r.jsx)(n.th,{children:"Spark JDBC connector configuration"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Since Version"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc-url"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"url"})}),(0,r.jsx)(n.td,{children:"JDBC URL for connecting to the database. For example, jdbc:mysql://localhost:3306"}),(0,r.jsx)(n.td,{children:"0.3.0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc-user"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc.user"})}),(0,r.jsx)(n.td,{children:"JDBC user name"}),(0,r.jsx)(n.td,{children:"0.3.0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc-password"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc.password"})}),(0,r.jsx)(n.td,{children:"JDBC password"}),(0,r.jsx)(n.td,{children:"0.3.0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"jdbc-driver"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"driver"})}),(0,r.jsx)(n.td,{children:"The driver of the JDBC connection. For example, com.mysql.jdbc.Driver or com.mysql.cj.jdbc.Driver"}),(0,r.jsx)(n.td,{children:"0.3.0"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["Gravitino catalog property names with the prefix ",(0,r.jsx)(n.code,{children:"spark.bypass."})," are passed to Spark JDBC connector."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);