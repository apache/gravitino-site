"use strict";(self.webpackChunkapache_website_template=self.webpackChunkapache_website_template||[]).push([[73471],{46164:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var s=n(13274),r=n(73325);const i={title:"Apache Gravitino Spark connector integration test",slug:"/spark-connector/spark-connector-integration-test",keyword:"spark connector integration test",license:"This software is licensed under the Apache License version 2."},c=void 0,o={id:"spark-connector/spark-integration-test",title:"Apache Gravitino Spark connector integration test",description:"Overview",source:"@site/docs/spark-connector/spark-integration-test.md",sourceDirName:"spark-connector",slug:"/spark-connector/spark-connector-integration-test",permalink:"/docs/next/spark-connector/spark-connector-integration-test",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/gravitino-site/tree/main/docs/spark-connector/spark-integration-test.md",tags:[],version:"current",frontMatter:{title:"Apache Gravitino Spark connector integration test",slug:"/spark-connector/spark-connector-integration-test",keyword:"spark connector integration test",license:"This software is licensed under the Apache License version 2."}},a={},d=[{value:"Overview",id:"overview",level:2},{value:"Normal integration test",id:"normal-integration-test",level:2},{value:"Golden file integration test",id:"golden-file-integration-test",level:2}];function l(e){const t={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(t.p,{children:["There are two types of integration tests in spark connector, normal integration test like ",(0,s.jsx)(t.code,{children:"SparkXXCatalogIT"}),", and the golden file integration test."]}),"\n",(0,s.jsx)(t.h2,{id:"normal-integration-test",children:"Normal integration test"}),"\n",(0,s.jsx)(t.p,{children:"Normal integration test are mainly used to test the correctness of the metadata, it's enabled in the GitHub CI. You could run tests with specific Spark version like:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:'./gradlew :spark-connector:spark-3.3:test --tests "org.apache.gravitino.spark.connector.integration.test.hive.SparkHiveCatalogIT33.testCreateHiveFormatPartitionTable"\n'})}),"\n",(0,s.jsx)(t.h2,{id:"golden-file-integration-test",children:"Golden file integration test"}),"\n",(0,s.jsx)(t.p,{children:"Golden file integration test are mainly to test the correctness of the SQL result with massive data, it's disabled in the GitHub CI, you could run tests with following command:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:'./gradlew :spark-connector:spark-3.3:test --tests  "org.apache.gravitino.spark.connector.integration.test.sql.SparkSQLRegressionTest33" -PenableSparkSQLITs\n'})}),"\n",(0,s.jsxs)(t.p,{children:["Please change the Spark version number if you want to test other Spark versions.\nIf you want to change the test behaviour, please modify ",(0,s.jsx)(t.code,{children:"spark-connector/spark-common/src/test/resources/spark-test.conf"}),"."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Configuration item"}),(0,s.jsx)(t.th,{children:"Description"}),(0,s.jsx)(t.th,{children:"Default value"}),(0,s.jsx)(t.th,{children:"Required"}),(0,s.jsx)(t.th,{children:"Since Version"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.dir"})}),(0,s.jsxs)(t.td,{children:["The Spark SQL test base dir, include ",(0,s.jsx)(t.code,{children:"test-sqls"})," and ",(0,s.jsx)(t.code,{children:"data"}),"."]}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"spark-connector/spark-common/src/test/resources/"})}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.sqls"})}),(0,s.jsxs)(t.td,{children:["Specify the test SQLs, using directory to specify group of SQLs like ",(0,s.jsx)(t.code,{children:"test-sqls/hive"}),", using file path to specify one SQL like ",(0,s.jsx)(t.code,{children:"test-sqls/hive/basic.sql"}),", use ",(0,s.jsx)(t.code,{children:","})," to split multi part"]}),(0,s.jsx)(t.td,{children:"run all SQLs"}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.generateGoldenFiles"})}),(0,s.jsx)(t.td,{children:"Whether generate golden files which are used to check the correctness of the SQL result"}),(0,s.jsx)(t.td,{children:"false"}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.metalake"})}),(0,s.jsx)(t.td,{children:"The metalake name to run the test"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"test"})}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.setupEnv"})}),(0,s.jsx)(t.td,{children:"Whether to setup Gravitino and Hive environment"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"false"})}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.uri"})}),(0,s.jsxs)(t.td,{children:["Gravitino uri address, only available when ",(0,s.jsx)(t.code,{children:"gravitino.spark.test.setupEnv"})," is false"]}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.a,{href:"http://127.0.0.1:8090",children:"http://127.0.0.1:8090"})}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"gravitino.spark.test.iceberg.warehouse"})}),(0,s.jsxs)(t.td,{children:["The warehouse location, only available when ",(0,s.jsx)(t.code,{children:"gravitino.spark.test.setupEnv"})," is false"]}),(0,s.jsx)(t.td,{children:"hdfs://127.0.0.1:9000/user/hive/warehouse-spark-test"}),(0,s.jsx)(t.td,{children:"No"}),(0,s.jsx)(t.td,{children:"0.6.0-incubating"})]})]})]}),"\n",(0,s.jsxs)(t.p,{children:["The test SQL files are located in ",(0,s.jsx)(t.code,{children:"spark-connector/spark-common/src/test/resources/"})," by default. There are three directories:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"hive"}),", SQL tests for Hive catalog."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"lakehouse-iceberg"}),", SQL tests for Iceberg catalog."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"tpcds"}),", SQL tests for ",(0,s.jsx)(t.code,{children:"tpcds"})," in Hive catalog."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["You could create a simple SQL file, like ",(0,s.jsx)(t.code,{children:"hive/catalog.sql"}),", the program will check the output with ",(0,s.jsx)(t.code,{children:"hive/catalog.sql.out"}),". For complex cases like ",(0,s.jsx)(t.code,{children:"tpcds"}),", you could do some prepare work like create table&load data in ",(0,s.jsx)(t.code,{children:"prepare.sql"}),"."]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},73325:(e,t,n)=>{n.d(t,{R:()=>c,x:()=>o});var s=n(79474);const r={},i=s.createContext(r);function c(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);