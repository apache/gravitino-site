"use strict";(globalThis.webpackChunkapache_website_template=globalThis.webpackChunkapache_website_template||[]).push([[97294],{49217(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var s=t(13274),i=t(25709);const a={title:"Fileset catalog with S3",slug:"/fileset-catalog-with-s3",date:new Date("2025-01-03T00:00:00.000Z"),keyword:"Fileset catalog S3",license:"This software is licensed under the Apache License version 2."},o=void 0,r={id:"fileset-catalog-with-s3",title:"Fileset catalog with S3",description:"This document explains how to configure a Fileset catalog with S3 in Gravitino.",source:"@site/versioned_docs/version-1.1.0/fileset-catalog-with-s3.md",sourceDirName:".",slug:"/fileset-catalog-with-s3",permalink:"/docs/1.1.0/fileset-catalog-with-s3",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/gravitino-site/tree/main/docs/fileset-catalog-with-s3.md",tags:[],version:"1.1.0",frontMatter:{title:"Fileset catalog with S3",slug:"/fileset-catalog-with-s3",date:"2025-01-03T00:00:00.000Z",keyword:"Fileset catalog S3",license:"This software is licensed under the Apache License version 2."},sidebar:"docs",previous:{title:"Fileset catalog",permalink:"/docs/1.1.0/fileset-catalog"},next:{title:"Fileset catalog with gcs",permalink:"/docs/1.1.0/fileset-catalog-with-gcs"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configurations for creating a Fileset catalog with S3",id:"configurations-for-creating-a-fileset-catalog-with-s3",level:2},{value:"Configurations for S3 Fileset Catalog",id:"configurations-for-s3-fileset-catalog",level:3},{value:"Configurations for a schema",id:"configurations-for-a-schema",level:3},{value:"Configurations for a fileset",id:"configurations-for-a-fileset",level:3},{value:"Using the Fileset catalog with S3",id:"using-the-fileset-catalog-with-s3",level:2},{value:"Step1: Create a Fileset Catalog with S3",id:"step1-create-a-fileset-catalog-with-s3",level:3},{value:"Step2: Create a schema",id:"step2-create-a-schema",level:3},{value:"Step3: Create a fileset",id:"step3-create-a-fileset",level:3},{value:"Accessing a fileset with S3",id:"accessing-a-fileset-with-s3",level:2},{value:"Using the GVFS Java client to access the fileset",id:"using-the-gvfs-java-client-to-access-the-fileset",level:3},{value:"Using Spark to access the fileset",id:"using-spark-to-access-the-fileset",level:3},{value:"Accessing a fileset using the Hadoop fs command",id:"accessing-a-fileset-using-the-hadoop-fs-command",level:3},{value:"Using the GVFS Python client to access a fileset",id:"using-the-gvfs-python-client-to-access-a-fileset",level:3},{value:"Using fileset with pandas",id:"using-fileset-with-pandas",level:3},{value:"Fileset with credential vending",id:"fileset-with-credential-vending",level:2},{value:"How to create a S3 Fileset catalog with credential vending",id:"how-to-create-a-s3-fileset-catalog-with-credential-vending",level:3},{value:"How to access S3 fileset with credential vending",id:"how-to-access-s3-fileset-with-credential-vending",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{TabItem:t,Tabs:a}=n;return t||p("TabItem",!0),a||p("Tabs",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This document explains how to configure a Fileset catalog with S3 in Gravitino."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"To create a Fileset catalog with S3, follow these steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Download the ",(0,s.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.gravitino/gravitino-aws-bundle",children:(0,s.jsx)(n.code,{children:"gravitino-aws-bundle-${gravitino-version}.jar"})})," file."]}),"\n",(0,s.jsxs)(n.li,{children:["Place this file in the Gravitino Fileset catalog classpath at ",(0,s.jsx)(n.code,{children:"${GRAVITINO_HOME}/catalogs/fileset/libs/"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Start the Gravitino server using the following command:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$ ${GRAVITINO_HOME}/bin/gravitino-server.sh start\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Once the server is up and running, you can proceed to configure the Fileset catalog with S3. In the rest of this document we will use ",(0,s.jsx)(n.code,{children:"http://localhost:8090"})," as the Gravitino server URL, please replace it with your actual server URL."]}),"\n",(0,s.jsx)(n.h2,{id:"configurations-for-creating-a-fileset-catalog-with-s3",children:"Configurations for creating a Fileset catalog with S3"}),"\n",(0,s.jsx)(n.h3,{id:"configurations-for-s3-fileset-catalog",children:"Configurations for S3 Fileset Catalog"}),"\n",(0,s.jsxs)(n.p,{children:["In addition to the basic configurations mentioned in ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/fileset-catalog#catalog-properties",children:"Fileset-catalog-catalog-configuration"}),", the following properties are necessary to configure a Fileset catalog with S3:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration item"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Since version"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"filesystem-providers"})}),(0,s.jsxs)(n.td,{children:["The file system providers to add. Set it to ",(0,s.jsx)(n.code,{children:"s3"})," if it's a S3 fileset, or a comma separated string that contains ",(0,s.jsx)(n.code,{children:"s3"})," like ",(0,s.jsx)(n.code,{children:"gs,s3"})," to support multiple kinds of fileset including ",(0,s.jsx)(n.code,{children:"s3"}),"."]}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"default-filesystem-provider"})}),(0,s.jsxs)(n.td,{children:["The name default filesystem providers of this Fileset catalog if users do not specify the scheme in the URI. Default value is ",(0,s.jsx)(n.code,{children:"builtin-local"}),", for S3, if we set this value, we can omit the prefix 's3a://' in the location."]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"builtin-local"})}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-endpoint"})}),(0,s.jsx)(n.td,{children:"The endpoint of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-access-key-id"})}),(0,s.jsx)(n.td,{children:"The access key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-secret-access-key"})}),(0,s.jsx)(n.td,{children:"The secret key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"credential-providers"})}),(0,s.jsxs)(n.td,{children:["The credential provider types, separated by comma, possible value can be ",(0,s.jsx)(n.code,{children:"s3-token"}),", ",(0,s.jsx)(n.code,{children:"s3-secret-key"}),". As the default authentication type is using AKSK as the above, this configuration can enable credential vending provided by Gravitino server and client will no longer need to provide authentication information like AKSK to access S3 by GVFS. Once it's set, more configuration items are needed to make it works, please see ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/security/credential-vending#s3-credentials",children:"s3-credential-vending"})]}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"0.8.0-incubating"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"configurations-for-a-schema",children:"Configurations for a schema"}),"\n",(0,s.jsxs)(n.p,{children:["To learn how to create a schema, refer to ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/fileset-catalog#schema-properties",children:"Schema configurations"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"configurations-for-a-fileset",children:"Configurations for a fileset"}),"\n",(0,s.jsxs)(n.p,{children:["For more details on creating a fileset, Refer to ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/fileset-catalog#fileset-properties",children:"Fileset configurations"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"using-the-fileset-catalog-with-s3",children:"Using the Fileset catalog with S3"}),"\n",(0,s.jsx)(n.p,{children:"This section demonstrates how to use the Fileset catalog with S3 in Gravitino, with a complete example."}),"\n",(0,s.jsx)(n.h3,{id:"step1-create-a-fileset-catalog-with-s3",children:"Step1: Create a Fileset Catalog with S3"}),"\n",(0,s.jsx)(n.p,{children:"First of all, you need to create a Fileset catalog with S3. The following example shows how to create a Fileset catalog with S3:"}),"\n",(0,s.jsxs)(a,{groupId:"language",queryString:!0,children:[(0,s.jsx)(t,{value:"shell",label:"Shell",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Accept: application/vnd.gravitino.v1+json" \\\n-H "Content-Type: application/json" -d \'{\n  "name": "test_catalog",\n  "type": "FILESET",\n  "comment": "This is a S3 fileset catalog",\n  "properties": {\n    "location": "s3a://bucket/root",\n    "s3-access-key-id": "access_key",\n    "s3-secret-access-key": "secret_key",\n    "s3-endpoint": "http://s3.ap-northeast-1.amazonaws.com",\n    "filesystem-providers": "s3"\n  }\n}\' http://localhost:8090/api/metalakes/metalake/catalogs\n'})})}),(0,s.jsx)(t,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'GravitinoClient gravitinoClient = GravitinoClient\n    .builder("http://localhost:8090")\n    .withMetalake("metalake")\n    .build();\n\nMap<String, String> s3Properties = ImmutableMap.<String, String>builder()\n    .put("location", "s3a://bucket/root")\n    .put("s3-access-key-id", "access_key")\n    .put("s3-secret-access-key", "secret_key")\n    .put("s3-endpoint", "http://s3.ap-northeast-1.amazonaws.com")\n    .put("filesystem-providers", "s3")\n    .build();\n\nCatalog s3Catalog = gravitinoClient.createCatalog("test_catalog",\n    Type.FILESET,\n    "This is a S3 fileset catalog",\n    s3Properties);\n// ...\n\n'})})}),(0,s.jsx)(t,{value:"python",label:"Python",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'gravitino_client: GravitinoClient = GravitinoClient(uri="http://localhost:8090", metalake_name="metalake")\ns3_properties = {\n    "location": "s3a://bucket/root",\n    "s3-access-key-id": "access_key"\n    "s3-secret-access-key": "secret_key",\n    "s3-endpoint": "http://s3.ap-northeast-1.amazonaws.com",\n    "filesystem-providers": "s3"\n}\n\ns3_catalog = gravitino_client.create_catalog(name="test_catalog",\n                                             catalog_type=Catalog.Type.FILESET,\n                                             provider=None,\n                                             comment="This is a S3 fileset catalog",\n                                             properties=s3_properties)\n'})})})]}),"\n",(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When using S3, ensure that the location value starts with s3a:// (not s3://) for AWS S3. For example, use s3a://bucket/root, as the s3:// format is not supported by the hadoop-aws library."}),"\n",(0,s.jsxs)(n.li,{children:["When using MinIO or other S3-compatible storage services, make sure to set the ",(0,s.jsx)(n.code,{children:"s3-endpoint"})," property to the appropriate endpoint URL."]}),"\n",(0,s.jsxs)(n.li,{children:["When using MinIO or other S3-compatible storage services, you may need to set additional properties such as ",(0,s.jsx)(n.code,{children:"s3-path-style-access"})," to ",(0,s.jsx)(n.code,{children:"true"})," depending on the storage service requirements. You can do this in Gravitino's ",(0,s.jsx)(n.code,{children:"fileset.conf"}),' file with the "gravitino.bypass." prefix:']}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"$ cat $GRAVITINO_HOME/catalogs/fileset/conf/fileset.conf\ngravitino.bypass.fs.s3a.path.style.access=true\n"})})]}),"\n",(0,s.jsx)(n.h3,{id:"step2-create-a-schema",children:"Step2: Create a schema"}),"\n",(0,s.jsx)(n.p,{children:"Once your Fileset catalog with S3 is created, you can create a schema under the catalog. Here are examples of how to do that:"}),"\n",(0,s.jsxs)(a,{groupId:"language",queryString:!0,children:[(0,s.jsx)(t,{value:"shell",label:"Shell",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Accept: application/vnd.gravitino.v1+json" \\\n-H "Content-Type: application/json" -d \'{\n  "name": "test_schema",\n  "comment": "This is a S3 schema",\n  "properties": {\n    "location": "s3a://bucket/root/schema"\n  }\n}\' http://localhost:8090/api/metalakes/metalake/catalogs/test_catalog/schemas\n'})})}),(0,s.jsx)(t,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'Catalog catalog = gravitinoClient.loadCatalog("hive_catalog");\n\nSupportsSchemas supportsSchemas = catalog.asSchemas();\n\nMap<String, String> schemaProperties = ImmutableMap.<String, String>builder()\n    .put("location", "s3a://bucket/root/schema")\n    .build();\nSchema schema = supportsSchemas.createSchema("test_schema",\n    "This is a S3 schema",\n    schemaProperties\n);\n// ...\n'})})}),(0,s.jsx)(t,{value:"python",label:"Python",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'gravitino_client: GravitinoClient = GravitinoClient(uri="http://localhost:8090", metalake_name="metalake")\ncatalog: Catalog = gravitino_client.load_catalog(name="test_catalog")\ncatalog.as_schemas().create_schema(name="test_schema",\n                                   comment="This is a S3 schema",\n                                   properties={"location": "s3a://bucket/root/schema"})\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"step3-create-a-fileset",children:"Step3: Create a fileset"}),"\n",(0,s.jsx)(n.p,{children:"After creating the schema, you can create a fileset. Here are examples for creating a fileset:"}),"\n",(0,s.jsxs)(a,{groupId:"language",queryString:!0,children:[(0,s.jsx)(t,{value:"shell",label:"Shell",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Accept: application/vnd.gravitino.v1+json" \\\n-H "Content-Type: application/json" -d \'{\n  "name": "example_fileset",\n  "comment": "This is an example fileset",\n  "type": "MANAGED",\n  "storageLocation": "s3a://bucket/root/schema/example_fileset",\n  "properties": {\n    "k1": "v1"\n  }\n}\' http://localhost:8090/api/metalakes/metalake/catalogs/test_catalog/schemas/test_schema/filesets\n'})})}),(0,s.jsx)(t,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'GravitinoClient gravitinoClient = GravitinoClient\n    .builder("http://localhost:8090")\n    .withMetalake("metalake")\n    .build();\n\nCatalog catalog = gravitinoClient.loadCatalog("test_catalog");\nFilesetCatalog filesetCatalog = catalog.asFilesetCatalog();\n\nMap<String, String> propertiesMap = ImmutableMap.<String, String>builder()\n      .put("k1", "v1")\n      .build();\n\nfilesetCatalog.createFileset(\n    NameIdentifier.of("test_schema", "example_fileset"),\n    "This is an example fileset",\n    Fileset.Type.MANAGED,\n    "s3a://bucket/root/schema/example_fileset",\n    propertiesMap\n);\n'})})}),(0,s.jsx)(t,{value:"python",label:"Python",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'gravitino_client: GravitinoClient = GravitinoClient(uri="http://localhost:8090", metalake_name="metalake")\n\ncatalog: Catalog = gravitino_client.load_catalog(name="catalog")\ncatalog.as_fileset_catalog().create_fileset(ident=NameIdentifier.of("schema", "example_fileset"),\n                                            type=Fileset.Type.MANAGED,\n                                            comment="This is an example fileset",\n                                            storage_location="s3a://bucket/root/schema/example_fileset",\n                                            properties={"k1": "v1"})\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"accessing-a-fileset-with-s3",children:"Accessing a fileset with S3"}),"\n",(0,s.jsx)(n.h3,{id:"using-the-gvfs-java-client-to-access-the-fileset",children:"Using the GVFS Java client to access the fileset"}),"\n",(0,s.jsxs)(n.p,{children:["To access fileset with S3 using the GVFS Java client, based on the ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/how-to-use-gvfs#configuration-1",children:"basic GVFS configurations"}),", you need to add the following configurations:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration item"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Since version"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-endpoint"})}),(0,s.jsx)(n.td,{children:"The endpoint of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-access-key-id"})}),(0,s.jsx)(n.td,{children:"The access key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3-secret-access-key"})}),(0,s.jsx)(n.td,{children:"The secret key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If the catalog has enabled ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/security/credential-vending",children:"credential vending"}),", the properties above can be omitted. More details can be found in ",(0,s.jsx)(n.a,{href:"#fileset-with-credential-vending",children:"Fileset with credential vending"}),"."]}),"\n"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'Configuration conf = new Configuration();\nconf.set("fs.AbstractFileSystem.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.Gvfs");\nconf.set("fs.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.GravitinoVirtualFileSystem");\nconf.set("fs.gravitino.server.uri", "http://localhost:8090");\nconf.set("fs.gravitino.client.metalake", "test_metalake");\nconf.set("s3-endpoint", "http://localhost:9000");\nconf.set("s3-access-key-id", "minio");\nconf.set("s3-secret-access-key", "minio123");\n\nPath filesetPath = new Path("gvfs://fileset/fileset_catalog/fileset_schema/my_fileset/new_dir");\nFileSystem fs = filesetPath.getFileSystem(conf);\nfs.mkdirs(filesetPath);\n...\n'})}),"\n",(0,s.jsx)(n.p,{children:"Similar to Spark configurations, you need to add S3 (bundle) jars to the classpath according to your environment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"  <dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-common</artifactId>\n    <version>${HADOOP_VERSION}</version>\n  </dependency>\n\n  <dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-aws</artifactId>\n    <version>${HADOOP_VERSION}</version>\n  </dependency>\n\n  <dependency>\n    <groupId>org.apache.gravitino</groupId>\n    <artifactId>gravitino-filesystem-hadoop3-runtime</artifactId>\n    <version>${GRAVITINO_VERSION}</version>\n  </dependency>\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["Since version 1.1.0, the ",(0,s.jsx)(n.code,{children:"gravitino-aws"})," JAR is no longer required, as it is now included in the ",(0,s.jsx)(n.code,{children:"gravitino-filesystem-hadoop3-runtime"})," JAR."]})}),"\n",(0,s.jsx)(n.p,{children:"Or use the bundle jar with Hadoop environment if there is no Hadoop environment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"  <dependency>\n    <groupId>org.apache.gravitino</groupId>\n    <artifactId>gravitino-aws-bundle</artifactId>\n    <version>${GRAVITINO_VERSION}</version>\n  </dependency>\n\n  <dependency>\n    <groupId>org.apache.gravitino</groupId>\n    <artifactId>gravitino-filesystem-hadoop3-runtime</artifactId>\n    <version>${GRAVITINO_VERSION}</version>\n  </dependency>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"using-spark-to-access-the-fileset",children:"Using Spark to access the fileset"}),"\n",(0,s.jsxs)(n.p,{children:["The following Python code demonstrates how to use ",(0,s.jsx)(n.strong,{children:"PySpark 3.5.0 with Hadoop environment(Hadoop 3.3.4)"})," to access the fileset:"]}),"\n",(0,s.jsx)(n.p,{children:"Before running the following code, you need to install required packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install pyspark==3.5.0\npip install apache-gravitino==${GRAVITINO_VERSION}\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then you can run the following code:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pyspark.sql import SparkSession\nimport os\n\ngravitino_url = "http://localhost:8090"\nmetalake_name = "test"\n\ncatalog_name = "your_s3_catalog"\nschema_name = "your_s3_schema"\nfileset_name = "your_s3_fileset"\n\n# JDK8 as follows, JDK17 will be slightly different, you need to add \'--conf \\"spark.driver.extraJavaOptions=--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\\" --conf \\"spark.executor.extraJavaOptions=--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\\"\' to the submit args.\nos.environ["PYSPARK_SUBMIT_ARGS"] = "--jars /path/to/gravitino-filesystem-hadoop3-runtime-${gravitino-version}-SNAPSHOT.jar,/path/to/hadoop-aws-3.3.4.jar,/path/to/aws-java-sdk-bundle-1.12.262.jar --master local[1] pyspark-shell"\nspark = SparkSession.builder\n    .appName("s3_fileset_test")\n    .config("spark.hadoop.fs.AbstractFileSystem.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.Gvfs")\n    .config("spark.hadoop.fs.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.GravitinoVirtualFileSystem")\n    .config("spark.hadoop.fs.gravitino.server.uri", "http://localhost:8090")\n    .config("spark.hadoop.fs.gravitino.client.metalake", "test")\n    .config("spark.hadoop.s3-access-key-id", os.environ["S3_ACCESS_KEY_ID"])\n    .config("spark.hadoop.s3-secret-access-key", os.environ["S3_SECRET_ACCESS_KEY"])\n    .config("spark.hadoop.s3-endpoint", "http://s3.ap-northeast-1.amazonaws.com")\n    .config("spark.driver.memory", "2g")\n    .config("spark.driver.port", "2048")\n    .getOrCreate()\n\ndata = [("Alice", 25), ("Bob", 30), ("Cathy", 45)]\ncolumns = ["Name", "Age"]\nspark_df = spark.createDataFrame(data, schema=columns)\ngvfs_path = f"gvfs://fileset/{catalog_name}/{schema_name}/{fileset_name}/people"\n\nspark_df.coalesce(1).write\n    .mode("overwrite")\n    .option("header", "true")\n    .csv(gvfs_path)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If your Spark ",(0,s.jsx)(n.strong,{children:"without Hadoop environment"}),", you can use the following code snippet to access the fileset:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'## Replace the following code snippet with the above code snippet with the same environment variables\nos.environ["PYSPARK_SUBMIT_ARGS"] = "--jars /path/to/gravitino-aws-bundle-${gravitino-version}.jar,/path/to/gravitino-filesystem-hadoop3-runtime-${gravitino-version}-SNAPSHOT.jar --master local[1] pyspark-shell"\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.gravitino/gravitino-aws-bundle",children:(0,s.jsx)(n.code,{children:"gravitino-aws-bundle-${gravitino-version}.jar"})}),': A "fat" JAR that includes ',(0,s.jsx)(n.code,{children:"gravitino-aws"})," functionality and all necessary dependencies like ",(0,s.jsx)(n.code,{children:"hadoop-aws"})," (3.3.1) and the ",(0,s.jsx)(n.code,{children:"AWS SDK"}),". Use this if your Spark environment doesn't have a pre-existing Hadoop setup."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.gravitino/gravitino-filesystem-hadoop3-runtime",children:(0,s.jsx)(n.code,{children:"gravitino-filesystem-hadoop3-runtime-${gravitino-version}.jar"})}),': A "fat" JAR that bundles Gravitino\'s virtual filesystem client and includes the functionality of ',(0,s.jsx)(n.code,{children:"gravitino-aws"}),". It is required for accessing Gravitino filesets."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"hadoop-aws-3.3.4.jar"})," and ",(0,s.jsx)(n.code,{children:"aws-java-sdk-bundle-1.12.262.jar"}),": Standard Hadoop dependencies for S3 access. If you are running in an existing Hadoop environment, you need to provide these JARs. They are typically located in the ",(0,s.jsx)(n.code,{children:"${HADOOP_HOME}/share/hadoop/tools/lib"})," directory."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.gravitino/gravitino-aws",children:(0,s.jsx)(n.code,{children:"gravitino-aws-${gravitino-version}.jar"})}),': A "thin" JAR that only provides the AWS integration code. Its functionality is already included in the ',(0,s.jsx)(n.code,{children:"gravitino-aws-bundle"})," and ",(0,s.jsx)(n.code,{children:"gravitino-filesystem-hadoop3-runtime"})," JARs, so you do not need to add it as a direct dependency unless you want to manage all Hadoop and AWS dependencies manually."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Please choose the correct jar according to your environment."}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"In some Spark versions, a Hadoop environment is needed by the driver, adding the bundle jars with '--jars' may not work. If this is the case, you should add the jars to the spark CLASSPATH directly."})}),"\n",(0,s.jsx)(n.h3,{id:"accessing-a-fileset-using-the-hadoop-fs-command",children:"Accessing a fileset using the Hadoop fs command"}),"\n",(0,s.jsxs)(n.p,{children:["The following are examples of how to use the ",(0,s.jsx)(n.code,{children:"hadoop fs"})," command to access the fileset in Hadoop 3.1.3."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Adding the following contents to the ",(0,s.jsx)(n.code,{children:"${HADOOP_HOME}/etc/hadoop/core-site.xml"})," file:"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"  <property>\n    <name>fs.AbstractFileSystem.gvfs.impl</name>\n    <value>org.apache.gravitino.filesystem.hadoop.Gvfs</value>\n  </property>\n\n  <property>\n    <name>fs.gvfs.impl</name>\n    <value>org.apache.gravitino.filesystem.hadoop.GravitinoVirtualFileSystem</value>\n  </property>\n\n  <property>\n    <name>fs.gravitino.server.uri</name>\n    <value>http://localhost:8090</value>\n  </property>\n\n  <property>\n    <name>fs.gravitino.client.metalake</name>\n    <value>test</value>\n  </property>\n\n  <property>\n    <name>s3-endpoint</name>\n    <value>http://s3.ap-northeast-1.amazonaws.com</value>\n  </property>\n\n  <property>\n    <name>s3-access-key-id</name>\n    <value>access-key</value>\n  </property>\n  \n  <property>\n  <name>s3-secret-access-key</name>\n    <value>secret-key</value>\n  </property>\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Add the necessary jars to the Hadoop classpath."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For S3, you need to add ",(0,s.jsx)(n.code,{children:"gravitino-filesystem-hadoop3-runtime-${gravitino-version}.jar"})," and ",(0,s.jsx)(n.code,{children:"hadoop-aws-${hadoop-version}.jar"})," located at ",(0,s.jsx)(n.code,{children:"${HADOOP_HOME}/share/hadoop/tools/lib/"})," to Hadoop classpath."]}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Run the following command to access the fileset:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"./${HADOOP_HOME}/bin/hadoop dfs -ls gvfs://fileset/s3_catalog/s3_schema/s3_fileset\n./${HADOOP_HOME}/bin/hadoop dfs -put /path/to/local/file gvfs://fileset/s3_catalog/s3_schema/s3_fileset\n"})}),"\n",(0,s.jsx)(n.h3,{id:"using-the-gvfs-python-client-to-access-a-fileset",children:"Using the GVFS Python client to access a fileset"}),"\n",(0,s.jsxs)(n.p,{children:["In order to access fileset with S3 using the GVFS Python client, apart from ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/how-to-use-gvfs#configuration-1",children:"basic GVFS configurations"}),", you need to add the following configurations:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration item"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Since version"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3_endpoint"})}),(0,s.jsx)(n.td,{children:"The endpoint of the AWS S3. This configuration is optional for S3 service, but required for other S3-compatible storage services like MinIO."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3_access_key_id"})}),(0,s.jsx)(n.td,{children:"The access key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"s3_secret_access_key"})}),(0,s.jsx)(n.td,{children:"The secret key of the AWS S3."}),(0,s.jsx)(n.td,{children:"(none)"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"0.7.0-incubating"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"s3_endpoint"})," is an optional configuration for GVFS ",(0,s.jsx)(n.strong,{children:"Python"})," client but a required configuration for GVFS ",(0,s.jsx)(n.strong,{children:"Java"})," client to access Hadoop with AWS S3, and it is required for other S3-compatible storage services like MinIO."]}),"\n",(0,s.jsxs)(n.li,{children:["If the catalog has enabled ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/security/credential-vending",children:"credential vending"}),", the properties above can be omitted."]}),"\n"]})}),"\n",(0,s.jsxs)(n.p,{children:["Please install the ",(0,s.jsx)(n.code,{children:"gravitino"})," package before running the following code:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install apache-gravitino==${GRAVITINO_VERSION}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from gravitino import gvfs\noptions = {\n    "cache_size": 20,\n    "cache_expired_time": 3600,\n    "auth_type": "simple",\n    "s3_endpoint": "http://localhost:9000",\n    "s3_access_key_id": "minio",\n    "s3_secret_access_key": "minio123"\n}\nfs = gvfs.GravitinoVirtualFileSystem(server_uri="http://localhost:8090", metalake_name="test_metalake", options=options)\nfs.ls("gvfs://fileset/{catalog_name}/{schema_name}/{fileset_name}/")                                                                         ")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-fileset-with-pandas",children:"Using fileset with pandas"}),"\n",(0,s.jsx)(n.p,{children:"The following are examples of how to use the pandas library to access the S3 fileset"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pandas as pd\n\nstorage_options = {\n    "server_uri": "http://localhost:8090", \n    "metalake_name": "test",\n    "options": {\n        "s3_access_key_id": "access_key",\n        "s3_secret_access_key": "secret_key",\n        "s3_endpoint": "http://s3.ap-northeast-1.amazonaws.com"\n    }\n}\nds = pd.read_csv(f"gvfs://fileset/${catalog_name}/${schema_name}/${fileset_name}/people/part-00000-51d366e2-d5eb-448d-9109-32a96c8a14dc-c000.csv",\n                 storage_options=storage_options)\nds.head()\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For more use cases, please refer to the ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/how-to-use-gvfs",children:"Gravitino Virtual File System"})," document."]}),"\n",(0,s.jsx)(n.h2,{id:"fileset-with-credential-vending",children:"Fileset with credential vending"}),"\n",(0,s.jsxs)(n.p,{children:["Since 0.8.0-incubating, Gravitino supports credential vending for S3 fileset. If the catalog has been ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/security/credential-vending",children:"configured with credential"}),", you can access S3 fileset without providing authentication information like ",(0,s.jsx)(n.code,{children:"s3-access-key-id"})," and ",(0,s.jsx)(n.code,{children:"s3-secret-access-key"})," in the properties."]}),"\n",(0,s.jsx)(n.h3,{id:"how-to-create-a-s3-fileset-catalog-with-credential-vending",children:"How to create a S3 Fileset catalog with credential vending"}),"\n",(0,s.jsxs)(n.p,{children:["Apart from configuration method in ",(0,s.jsx)(n.a,{href:"#configurations-for-s3-fileset-catalog",children:"create-s3-fileset-catalog"}),",\nproperties needed by ",(0,s.jsx)(n.a,{href:"/docs/1.1.0/security/credential-vending#s3-credentials",children:"s3-credential"}),"\nshould also be set to enable credential vending for S3 fileset. Take ",(0,s.jsx)(n.code,{children:"s3-token"})," credential provider for example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Accept: application/vnd.gravitino.v1+json" \\\n-H "Content-Type: application/json" -d \'{\n  "name": "s3-catalog-with-token",\n  "type": "FILESET",\n  "comment": "This is a S3 fileset catalog",\n  "properties": {\n    "location": "s3a://bucket/root",\n    "s3-access-key-id": "access_key",\n    "s3-secret-access-key": "secret_key",\n    "s3-endpoint": "http://s3.ap-northeast-1.amazonaws.com",\n    "filesystem-providers": "s3",\n    "credential-providers": "s3-token",\n    "s3-region":"ap-northeast-1",\n    "s3-role-arn":"The ARN of the role to access the S3 data"\n  }\n}\' http://localhost:8090/api/metalakes/metalake/catalogs\n'})}),"\n",(0,s.jsx)(n.h3,{id:"how-to-access-s3-fileset-with-credential-vending",children:"How to access S3 fileset with credential vending"}),"\n",(0,s.jsx)(n.p,{children:"When the catalog is configured with credentials and client-side credential vending is enabled,\nyou can access S3 filesets directly using the GVFS Java/Python client or Spark without providing authentication details."}),"\n",(0,s.jsx)(n.p,{children:"GVFS Java client:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'Configuration conf = new Configuration();\nconf.setBoolean("fs.gravitino.enableCredentialVending", true);\nconf.set("fs.AbstractFileSystem.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.Gvfs");\nconf.set("fs.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.GravitinoVirtualFileSystem");\nconf.set("fs.gravitino.server.uri", "http://localhost:8090");\nconf.set("fs.gravitino.client.metalake", "test_metalake");\n// No need to set s3-access-key-id and s3-secret-access-key\nPath filesetPath = new Path("gvfs://fileset/test_catalog/test_schema/test_fileset/new_dir");\nFileSystem fs = filesetPath.getFileSystem(conf);\nfs.mkdirs(filesetPath);\n...\n'})}),"\n",(0,s.jsx)(n.p,{children:"Spark:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'spark = SparkSession.builder\n    .appName("s3_fileset_test")\n    .config("spark.hadoop.fs.gravitino.enableCredentialVending", "true")\n    .config("spark.hadoop.fs.AbstractFileSystem.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.Gvfs")\n    .config("spark.hadoop.fs.gvfs.impl", "org.apache.gravitino.filesystem.hadoop.GravitinoVirtualFileSystem")\n    .config("spark.hadoop.fs.gravitino.server.uri", "http://localhost:8090")\n    .config("spark.hadoop.fs.gravitino.client.metalake", "test")\n    # No need to set s3-access-key-id and s3-secret-access-key\n    .config("spark.driver.memory", "2g")\n    .config("spark.driver.port", "2048")\n    .getOrCreate()\n'})}),"\n",(0,s.jsx)(n.p,{children:"Python client and Hadoop command are similar to the above examples."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}function p(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);